{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohsenJadidi/Automatic-Modulation-Classification-AMC/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0dBrWRWh7UO",
        "colab_type": "code",
        "outputId": "f33ba2c0-ecb7-40fe-e40b-7cebccfcdc71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR0ATiKBk6rG",
        "colab_type": "text"
      },
      "source": [
        "# Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfzzfiDSuxMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "fileName = 'RML2016.10a_dict.pkl'\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/\"+fileName,'rb') as f:\n",
        "  data = pickle.load(f,encoding='bytes')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjAg9T-0uzyh",
        "colab_type": "code",
        "outputId": "65538990-5823-4f00-bf83-d61477b513cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "X = []\n",
        "labels = [] # label each example by a pair (modulation type, snr)\n",
        "total_examples = 0\n",
        "analog = [b'AM-DSB', b'AM-SSB', b'WBFM']\n",
        "\n",
        "for mod_type, snr in data.keys():\n",
        "    if (mod_type not in analog):      \n",
        "        current_matrix = data[(mod_type, snr)]        \n",
        "        total_examples += current_matrix.shape[0]\n",
        "        for i in range(current_matrix.shape[0]):\n",
        "            X.append(current_matrix[i])\n",
        "            labels.append((str(mod_type, 'ascii'), snr)) # mod_type is of type bytes\n",
        "    \n",
        "X = np.array(X)         # First row is QPSK snr=2, seconde is PAM4 snr=8 , ...\n",
        "labels = np.array(labels)\n",
        "\n",
        "y = labels[:,0]\n",
        "\n",
        "print(f'loaded {total_examples} signal vectors into X{X.shape} and their corresponding'\n",
        "      f' labels into labels{labels.shape}')  \n",
        "# print(np.unique(labels[:,0]))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded 160000 signal vectors into X(160000, 2, 128) and their corresponding labels into labels(160000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbeDr3FVv57v",
        "colab_type": "code",
        "outputId": "3a5bc591-9fd6-4f24-fc5a-01a3e4eb7c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Encoding categorical data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_y = LabelEncoder()\n",
        "y = labelencoder_y.fit_transform(y)\n",
        "onehotencoder = OneHotEncoder()\n",
        "y = onehotencoder.fit_transform(y.reshape(-1,1)).toarray()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6iUKbJwKlT",
        "colab_type": "code",
        "outputId": "76897695-1a7e-49ca-c425-397b05345a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "snrList = [str(2*i-20) for i in range(20)]  # snrList = -20, -18, -16 , ... ,0, ... ,18\n",
        "snr = snrList[19]\n",
        "numberOfEachExamples = 1000\n",
        "print(\"SNR :\", snr)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR : 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DiHGfSPwKb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = [[labels[i*numberOfEachExamples, 0],y[i*numberOfEachExamples]] for i in range(int(X.shape[0]/numberOfEachExamples))]\n",
        "output = dict(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OJ3GghKwrbs",
        "colab_type": "code",
        "outputId": "415ffb22-dba1-4b5a-f815-7b96be1a6de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xsnr = np.zeros(shape=(X.shape[0],X.shape[1],X.shape[2]+1))\n",
        "for i in range(X.shape[0]):\n",
        "    snr = int(labels[i,1])\n",
        "    Xsnr[i,0,:] = np.insert(X[i,0,:],0,snr)\n",
        "    Xsnr[i,1,:] = np.insert(X[i,1,:],0,snr)\n",
        "    \n",
        "print(Xsnr.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(160000, 2, 129)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx7l0G_3ww0w",
        "colab_type": "code",
        "outputId": "ba862a72-d16d-4fd9-f476-840c161f6d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "###### Splitting the dataset into the Training set and Test set ######\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xsnr, y, test_size = 0.2, random_state = 0)\n",
        "# The below line better for Cross_val part\n",
        "#X_train, X_test, y_train, y_test = train_test_split(Xsnr, y, test_size = 1, random_state = 0)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128000, 2, 129)\n",
            "(32000, 2, 129)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGeDKe8GLNJ9",
        "colab_type": "code",
        "outputId": "edb761e3-33c6-4b25-ff1f-fef74edb3693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X_train = X_train[:,:,1:] # snr important for train\n",
        "\n",
        "y_test18 = []\n",
        "X_test18 = []\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    if X_test[i,0,0] == 18:\n",
        "        X_test18.append(X_test[i])\n",
        "        y_test18.append(y_test[i])\n",
        "        \n",
        "X_test18 = np.array(X_test18)\n",
        "y_test18 = np.array(y_test18)        \n",
        "X_test18 = X_test18[:,:,1:]\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test18.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128000, 2, 128)\n",
            "(1653, 2, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4OinfCHLd8y",
        "colab_type": "code",
        "outputId": "55d95c1e-359c-4aec-d803-ecf2cc1c7655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Change IQ to amplitude and phase\n",
        "X_cmplx = X_train[:,0,:] + 1j* X_train[:,1,:]    \n",
        "X_amp = np.abs(X_cmplx)\n",
        "X_ang = np.arctan2(X_train[:,1,:],X_train[:,0,:]) / np.pi\n",
        "    \n",
        "X_amp = np.reshape(X_amp,(-1,1,128))\n",
        "X_ang = np.reshape(X_ang,(-1,1,128))\n",
        "    \n",
        "X_train_AmpPhs = np.concatenate((X_amp,X_ang), axis=1) \n",
        "##\n",
        "X_cmplx = X_test18[:,0,:] + 1j* X_test18[:,1,:]    \n",
        "X_amp = np.abs(X_cmplx)\n",
        "X_ang = np.arctan2(X_test18[:,1,:],X_test18[:,0,:]) / np.pi\n",
        "    \n",
        "X_amp = np.reshape(X_amp,(-1,1,128))\n",
        "X_ang = np.reshape(X_ang,(-1,1,128))\n",
        "    \n",
        "X_test18_AmpPhs = np.concatenate((X_amp,X_ang), axis=1) \n",
        "##\n",
        "\n",
        "print(X_train_AmpPhs.shape)\n",
        "print(X_test18_AmpPhs.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128000, 2, 128)\n",
            "(1653, 2, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujxapqcRw1l3",
        "colab_type": "code",
        "outputId": "a7368bc4-0ca6-4936-f320-d00bd82f2513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = X_train.reshape([X_train.shape[0],256])\n",
        "#X_train = X_train_AmpPhs.reshape([X_train.shape[0],256])\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_train = X_train.reshape([X_train.shape[0],2,128])\n",
        "#X_test = X_test.reshape([1600,256])\n",
        "X_test18 = X_test18.reshape([X_test18.shape[0],256])\n",
        "#X_test18 = X_test18_AmpPhs.reshape([X_test18.shape[0],256])\n",
        "X_test18 = sc.transform(X_test18)\n",
        "X_test18 = X_test18.reshape([X_test18.shape[0],2,128])\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(-1,2, 128, 1)   #Reshape for CNN -  (6400,2,128)->(6400,2,128,1)!!\n",
        "X_test18 = X_test18.reshape(-1,2, 128, 1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test18.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128000, 2, 128, 1)\n",
            "(1653, 2, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5InZvNieUoTS",
        "colab_type": "text"
      },
      "source": [
        "# Making CNN0 model \n",
        "(Article : Convolutional Radio Modulation Recognition Networks - Timothy J. O’Shea)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ida-kcQ9Ub0f",
        "colab_type": "code",
        "outputId": "1bce2e1d-e7cc-44c9-a173-9de36b4e92b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "dout = 0.5\n",
        "classifier.add(Conv2D(filters=64, kernel_size=(1,3), input_shape =  (2,128,1), padding='same', activation = 'relu')) \n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(filters=64,kernel_size=(1,3),  padding='same', activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(filters=64,kernel_size=(1,3),  padding='same', activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(filters=64,kernel_size=(2,3),  padding='same', activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(output_dim = 128 , activation = 'relu')) # hidden layer\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Dense(output_dim = 8 , activation = 'softmax'))\n",
        "\n",
        "#from keras import optimizers\n",
        "#adamOpt = optimizers.adam(lr = 0.01)\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#logger = keras.callbacks.TensorBoard(log_dir=\"./logs\", write_graph=True, histogram_freq=0)\n",
        "\n",
        "classifier.summary()\n",
        "print(\"Model Created!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 2, 128, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 2, 128, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 2, 128, 64)        12352     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 2, 128, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 128, 64)        12352     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 2, 128, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 2, 128, 64)        24640     \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 2, 128, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               2097280   \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 2,147,912\n",
            "Trainable params: 2,147,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model Created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=8)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAqdPV-N3Ae_",
        "colab_type": "text"
      },
      "source": [
        "#Making CNN1 model (one-convolutional-layer)\n",
        "(Thesis :DEEP NEURAL NETWORK ARCHITECTURES FOR MODULATION CLASSIFICATION)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaOTxksa3D2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "dout = 0.5\n",
        "\n",
        "classifier.add(Conv2D(256,1,3, input_shape =  (2,128,1), activation = 'relu')) # 256=number of filter\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(output_dim = 128 , activation = 'relu')) # hidden layer\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Dense(output_dim = 8 , activation = 'softmax'))\n",
        "\n",
        "#from keras import optimizers\n",
        "#adamOpt = optimizers.adam(lr = 0.01)\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#logger = keras.callbacks.TensorBoard(log_dir=\"./logs\", write_graph=True, histogram_freq=0)\n",
        "\n",
        "classifier.summary()\n",
        "print(\"Model Created!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2HVRLuRvC2h",
        "colab_type": "text"
      },
      "source": [
        "# Making CNN2 model  (two-convolutional-layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EPV9BVqvAZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "dout = 0.4\n",
        "classifier.add(Conv2D(256,(1,3), input_shape =  (2,128,1), activation = 'relu', padding='same')) # 256=number of filter\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(80,(2,3), activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(output_dim = 128 , activation = 'relu')) # hidden layer\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Dense(output_dim = 8 , activation = 'softmax'))\n",
        "\n",
        "#from keras import optimizers\n",
        "#adamOpt = optimizers.adam(lr = 0.01)\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#logger = keras.callbacks.TensorBoard(log_dir=\"./logs\", write_graph=True, histogram_freq=0)\n",
        "\n",
        "classifier.summary()\n",
        "print(\"Model Created!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFwbq2eup-98",
        "colab_type": "text"
      },
      "source": [
        "#Making CNN3 model (two-convolutional-layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EOnbMLu3Qpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "dout = 0.4\n",
        "\n",
        "classifier.add(Conv2D(80,2,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Conv2D(256,1,3, input_shape =  (2,128,1), activation = 'relu')) # 256=number of filter\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(output_dim = 128 , activation = 'relu')) # hidden layer\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Dense(output_dim = 8 , activation = 'softmax'))\n",
        "\n",
        "#from keras import optimizers\n",
        "#adamOpt = optimizers.adam(lr = 0.01)\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#logger = keras.callbacks.TensorBoard(log_dir=\"./logs\", write_graph=True, histogram_freq=0)\n",
        "\n",
        "classifier.summary()\n",
        "print(\"Model Created!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcI2TeYzEwY6",
        "colab_type": "text"
      },
      "source": [
        "#Making CNN4 model (four-convolutional-layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EcNw2P4Ew_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "dout = 0.4\n",
        "classifier.add(Conv2D(256,1,3, input_shape =  (2,128,1), activation = 'relu')) # 256=number of filter\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(256,2,3, activation = 'relu')) # 256=number of filter\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Conv2D(80,1,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(80,1,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(output_dim = 128 , activation = 'relu')) # hidden layer\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Dense(output_dim = 8 , activation = 'softmax'))\n",
        "\n",
        "#from keras import optimizers\n",
        "#adamOpt = optimizers.adam(lr = 0.01)\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#logger = keras.callbacks.TensorBoard(log_dir=\"./logs\", write_graph=True, histogram_freq=0)\n",
        "\n",
        "classifier.summary()\n",
        "print(\"Model Created!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv1KiftgFi9w",
        "colab_type": "text"
      },
      "source": [
        "#Making CNN5 model (five-convolutional-layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DceZWd-aFjkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "dout = 0.4\n",
        "classifier.add(Conv2D(384,1,3, input_shape =  (2,128,1), activation = 'relu')) \n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(256,2,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(80,1,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(256,1,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Conv2D(80,1,3, activation = 'relu'))\n",
        "classifier.add(Dropout(rate = dout))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "\n",
        "classifier.add(Dense(output_dim = 128 , activation = 'relu')) # hidden layer\n",
        "classifier.add(Dropout(rate = dout))\n",
        "classifier.add(Dense(output_dim = 8 , activation = 'softmax'))\n",
        "\n",
        "#from keras import optimizers\n",
        "#adamOpt = optimizers.adam(lr = 0.01)\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#logger = keras.callbacks.TensorBoard(log_dir=\"./logs\", write_graph=True, histogram_freq=0)\n",
        "\n",
        "classifier.summary()\n",
        "print(\"Model Created!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct6SfO0K3RPR",
        "colab_type": "text"
      },
      "source": [
        "# Fitting model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTK9sEg7uiRA",
        "colab_type": "code",
        "outputId": "9f514adc-7648-4177-fcfc-51f363e225e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch = 1024\n",
        "epoch = 100\n",
        "\n",
        "history = classifier.fit(X_train, y_train, batch_size = batch, epochs = epoch, validation_data=(X_test18, y_test18))\n",
        "#history = classifier.fit(X_train, y_train, batch_size = batch, epochs = epoch)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 128000 samples, validate on 1653 samples\n",
            "Epoch 1/100\n",
            "128000/128000 [==============================] - 23s 176us/step - loss: 1.9684 - acc: 0.1802 - val_loss: 1.6354 - val_acc: 0.2880\n",
            "Epoch 2/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.8455 - acc: 0.2192 - val_loss: 1.5035 - val_acc: 0.3406\n",
            "Epoch 3/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.7990 - acc: 0.2388 - val_loss: 1.3985 - val_acc: 0.4858\n",
            "Epoch 4/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.6541 - acc: 0.3050 - val_loss: 1.0268 - val_acc: 0.5439\n",
            "Epoch 5/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.5504 - acc: 0.3480 - val_loss: 0.9116 - val_acc: 0.5771\n",
            "Epoch 6/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.4977 - acc: 0.3639 - val_loss: 0.8467 - val_acc: 0.5917\n",
            "Epoch 7/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.4875 - acc: 0.3732 - val_loss: 0.7576 - val_acc: 0.6304\n",
            "Epoch 8/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.4606 - acc: 0.3849 - val_loss: 0.7559 - val_acc: 0.6455\n",
            "Epoch 9/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.4410 - acc: 0.3916 - val_loss: 0.6775 - val_acc: 0.6770\n",
            "Epoch 10/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.4284 - acc: 0.3967 - val_loss: 0.6572 - val_acc: 0.6848\n",
            "Epoch 11/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.4174 - acc: 0.4041 - val_loss: 0.6151 - val_acc: 0.7145\n",
            "Epoch 12/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.3856 - acc: 0.4150 - val_loss: 0.4789 - val_acc: 0.7816\n",
            "Epoch 13/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.3509 - acc: 0.4287 - val_loss: 0.4616 - val_acc: 0.7641\n",
            "Epoch 14/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.3283 - acc: 0.4388 - val_loss: 0.4361 - val_acc: 0.7840\n",
            "Epoch 15/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.3155 - acc: 0.4440 - val_loss: 0.4026 - val_acc: 0.7943\n",
            "Epoch 16/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.3068 - acc: 0.4488 - val_loss: 0.4152 - val_acc: 0.7901\n",
            "Epoch 17/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.3034 - acc: 0.4507 - val_loss: 0.3976 - val_acc: 0.7925\n",
            "Epoch 18/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.3005 - acc: 0.4514 - val_loss: 0.4295 - val_acc: 0.7822\n",
            "Epoch 19/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2910 - acc: 0.4529 - val_loss: 0.4086 - val_acc: 0.7762\n",
            "Epoch 20/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2844 - acc: 0.4543 - val_loss: 0.4084 - val_acc: 0.7798\n",
            "Epoch 21/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2796 - acc: 0.4559 - val_loss: 0.4072 - val_acc: 0.7858\n",
            "Epoch 22/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2782 - acc: 0.4575 - val_loss: 0.4051 - val_acc: 0.7786\n",
            "Epoch 23/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2740 - acc: 0.4598 - val_loss: 0.4008 - val_acc: 0.7937\n",
            "Epoch 24/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2707 - acc: 0.4608 - val_loss: 0.4086 - val_acc: 0.7774\n",
            "Epoch 25/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2693 - acc: 0.4612 - val_loss: 0.4061 - val_acc: 0.7774\n",
            "Epoch 26/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2678 - acc: 0.4610 - val_loss: 0.3905 - val_acc: 0.7901\n",
            "Epoch 27/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2639 - acc: 0.4607 - val_loss: 0.4006 - val_acc: 0.7604\n",
            "Epoch 28/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2572 - acc: 0.4649 - val_loss: 0.3781 - val_acc: 0.7937\n",
            "Epoch 29/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2537 - acc: 0.4662 - val_loss: 0.3864 - val_acc: 0.7737\n",
            "Epoch 30/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2497 - acc: 0.4683 - val_loss: 0.3793 - val_acc: 0.7985\n",
            "Epoch 31/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2474 - acc: 0.4696 - val_loss: 0.3811 - val_acc: 0.7804\n",
            "Epoch 32/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2471 - acc: 0.4695 - val_loss: 0.3808 - val_acc: 0.7901\n",
            "Epoch 33/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2419 - acc: 0.4716 - val_loss: 0.3739 - val_acc: 0.7949\n",
            "Epoch 34/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2407 - acc: 0.4722 - val_loss: 0.3651 - val_acc: 0.8052\n",
            "Epoch 35/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2373 - acc: 0.4741 - val_loss: 0.3965 - val_acc: 0.8028\n",
            "Epoch 36/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2386 - acc: 0.4724 - val_loss: 0.3552 - val_acc: 0.8040\n",
            "Epoch 37/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2337 - acc: 0.4774 - val_loss: 0.3553 - val_acc: 0.8040\n",
            "Epoch 38/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2295 - acc: 0.4776 - val_loss: 0.3575 - val_acc: 0.8040\n",
            "Epoch 39/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2289 - acc: 0.4768 - val_loss: 0.3750 - val_acc: 0.7937\n",
            "Epoch 40/100\n",
            "128000/128000 [==============================] - 22s 168us/step - loss: 1.2278 - acc: 0.4782 - val_loss: 0.3566 - val_acc: 0.8010\n",
            "Epoch 41/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.2162 - acc: 0.4818 - val_loss: 0.3456 - val_acc: 0.8046\n",
            "Epoch 42/100\n",
            "128000/128000 [==============================] - 22s 168us/step - loss: 1.2094 - acc: 0.4824 - val_loss: 0.3573 - val_acc: 0.7985\n",
            "Epoch 43/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2086 - acc: 0.4828 - val_loss: 0.3455 - val_acc: 0.8100\n",
            "Epoch 44/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2037 - acc: 0.4836 - val_loss: 0.3531 - val_acc: 0.8028\n",
            "Epoch 45/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.2008 - acc: 0.4855 - val_loss: 0.3531 - val_acc: 0.7979\n",
            "Epoch 46/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1948 - acc: 0.4859 - val_loss: 0.3404 - val_acc: 0.7992\n",
            "Epoch 47/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1905 - acc: 0.4880 - val_loss: 0.3384 - val_acc: 0.8131\n",
            "Epoch 48/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1853 - acc: 0.4881 - val_loss: 0.3381 - val_acc: 0.8064\n",
            "Epoch 49/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1859 - acc: 0.4885 - val_loss: 0.3440 - val_acc: 0.8028\n",
            "Epoch 50/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1788 - acc: 0.4895 - val_loss: 0.3297 - val_acc: 0.8113\n",
            "Epoch 51/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1816 - acc: 0.4891 - val_loss: 0.3413 - val_acc: 0.8076\n",
            "Epoch 52/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1797 - acc: 0.4895 - val_loss: 0.3402 - val_acc: 0.8034\n",
            "Epoch 53/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1786 - acc: 0.4903 - val_loss: 0.3298 - val_acc: 0.8125\n",
            "Epoch 54/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1762 - acc: 0.4907 - val_loss: 0.3295 - val_acc: 0.8179\n",
            "Epoch 55/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1754 - acc: 0.4915 - val_loss: 0.3283 - val_acc: 0.8137\n",
            "Epoch 56/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1729 - acc: 0.4919 - val_loss: 0.3262 - val_acc: 0.8155\n",
            "Epoch 57/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1728 - acc: 0.4917 - val_loss: 0.3304 - val_acc: 0.8149\n",
            "Epoch 58/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1713 - acc: 0.4936 - val_loss: 0.3265 - val_acc: 0.8064\n",
            "Epoch 59/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1714 - acc: 0.4914 - val_loss: 0.3261 - val_acc: 0.8161\n",
            "Epoch 60/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1635 - acc: 0.4942 - val_loss: 0.3215 - val_acc: 0.8076\n",
            "Epoch 61/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1604 - acc: 0.4954 - val_loss: 0.3276 - val_acc: 0.8246\n",
            "Epoch 62/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1582 - acc: 0.4946 - val_loss: 0.3256 - val_acc: 0.8155\n",
            "Epoch 63/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1548 - acc: 0.4963 - val_loss: 0.3179 - val_acc: 0.8143\n",
            "Epoch 64/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1563 - acc: 0.4955 - val_loss: 0.3127 - val_acc: 0.8270\n",
            "Epoch 65/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1561 - acc: 0.4973 - val_loss: 0.3172 - val_acc: 0.8227\n",
            "Epoch 66/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1523 - acc: 0.4974 - val_loss: 0.3182 - val_acc: 0.8197\n",
            "Epoch 67/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1523 - acc: 0.4979 - val_loss: 0.3143 - val_acc: 0.8234\n",
            "Epoch 68/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1525 - acc: 0.4995 - val_loss: 0.3103 - val_acc: 0.8294\n",
            "Epoch 69/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1463 - acc: 0.5016 - val_loss: 0.3088 - val_acc: 0.8246\n",
            "Epoch 70/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1478 - acc: 0.5016 - val_loss: 0.3036 - val_acc: 0.8306\n",
            "Epoch 71/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1471 - acc: 0.5018 - val_loss: 0.3037 - val_acc: 0.8355\n",
            "Epoch 72/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1451 - acc: 0.5022 - val_loss: 0.3019 - val_acc: 0.8294\n",
            "Epoch 73/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1455 - acc: 0.5043 - val_loss: 0.3030 - val_acc: 0.8215\n",
            "Epoch 74/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1436 - acc: 0.5046 - val_loss: 0.2982 - val_acc: 0.8282\n",
            "Epoch 75/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1405 - acc: 0.5047 - val_loss: 0.2907 - val_acc: 0.8361\n",
            "Epoch 76/100\n",
            "128000/128000 [==============================] - 22s 168us/step - loss: 1.1387 - acc: 0.5061 - val_loss: 0.2952 - val_acc: 0.8294\n",
            "Epoch 77/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.1394 - acc: 0.5061 - val_loss: 0.2945 - val_acc: 0.8330\n",
            "Epoch 78/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1399 - acc: 0.5067 - val_loss: 0.2893 - val_acc: 0.8318\n",
            "Epoch 79/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1361 - acc: 0.5071 - val_loss: 0.2901 - val_acc: 0.8342\n",
            "Epoch 80/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1336 - acc: 0.5101 - val_loss: 0.2902 - val_acc: 0.8409\n",
            "Epoch 81/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1354 - acc: 0.5100 - val_loss: 0.2800 - val_acc: 0.8433\n",
            "Epoch 82/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1337 - acc: 0.5088 - val_loss: 0.2783 - val_acc: 0.8391\n",
            "Epoch 83/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1319 - acc: 0.5111 - val_loss: 0.2843 - val_acc: 0.8379\n",
            "Epoch 84/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1277 - acc: 0.5110 - val_loss: 0.2913 - val_acc: 0.8318\n",
            "Epoch 85/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1264 - acc: 0.5129 - val_loss: 0.2742 - val_acc: 0.8367\n",
            "Epoch 86/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1185 - acc: 0.5169 - val_loss: 0.2648 - val_acc: 0.8524\n",
            "Epoch 87/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1149 - acc: 0.5155 - val_loss: 0.2633 - val_acc: 0.8403\n",
            "Epoch 88/100\n",
            "128000/128000 [==============================] - 22s 168us/step - loss: 1.1132 - acc: 0.5178 - val_loss: 0.2700 - val_acc: 0.8512\n",
            "Epoch 89/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1125 - acc: 0.5175 - val_loss: 0.2757 - val_acc: 0.8415\n",
            "Epoch 90/100\n",
            "128000/128000 [==============================] - 22s 168us/step - loss: 1.1073 - acc: 0.5201 - val_loss: 0.2715 - val_acc: 0.8415\n",
            "Epoch 91/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1043 - acc: 0.5199 - val_loss: 0.2740 - val_acc: 0.8361\n",
            "Epoch 92/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1010 - acc: 0.5218 - val_loss: 0.2714 - val_acc: 0.8445\n",
            "Epoch 93/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1036 - acc: 0.5202 - val_loss: 0.2683 - val_acc: 0.8445\n",
            "Epoch 94/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.1005 - acc: 0.5239 - val_loss: 0.2781 - val_acc: 0.8379\n",
            "Epoch 95/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.0963 - acc: 0.5240 - val_loss: 0.2647 - val_acc: 0.8427\n",
            "Epoch 96/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.0971 - acc: 0.5222 - val_loss: 0.2573 - val_acc: 0.8494\n",
            "Epoch 97/100\n",
            "128000/128000 [==============================] - 21s 167us/step - loss: 1.0980 - acc: 0.5223 - val_loss: 0.2678 - val_acc: 0.8421\n",
            "Epoch 98/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.0962 - acc: 0.5229 - val_loss: 0.2606 - val_acc: 0.8506\n",
            "Epoch 99/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.0939 - acc: 0.5239 - val_loss: 0.2623 - val_acc: 0.8494\n",
            "Epoch 100/100\n",
            "128000/128000 [==============================] - 21s 168us/step - loss: 1.0956 - acc: 0.5244 - val_loss: 0.2538 - val_acc: 0.8554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKlbDvMkD6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFgwssVfxtd",
        "colab_type": "text"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKe0m_39HiWU",
        "colab_type": "code",
        "outputId": "46e9c99f-6e50-4761-f574-d794b65e6377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "dic = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "classifier.save(dic+f'SaveModel/CNN3-64(1,3)-64(1,3)-64(1,3)-64(2,3)-ba{batch}-ep{epoch}-(dout=0.5)-input(IQ)(SNR=all).h5')\n",
        "f = open(dic+f'SaveModel/CNN3-64(1,3)-64(1,3)-64(1,3)-64(2,3)-ba{batch}-ep{epoch}-(dout=0.5)-input(IQ)(SNR=all)-history.txt',\"w\")\n",
        "f.write( str(classifier.history.history) )\n",
        "f.close()\n",
        "\n",
        "print('Model Saved!')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNGu7HtyhGc2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Prediction(only SNR=18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6irYYHCZ6A2C",
        "colab_type": "code",
        "outputId": "0c97d29a-7e7a-4ca7-b475-755aa83278d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "y_pred = classifier.predict(X_test18)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "y_real = np.argmax(y_test18, axis=1)\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_real, y_pred)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "acc_test = classifier.evaluate(X_test18, y_test18)[1]\n",
        "acc_train = classifier.evaluate(X_train, y_train)[1]\n",
        "\n",
        "print(\"Acc Test : \", acc_test)\n",
        "print(\"Acc Train : \", acc_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1653/1653 [==============================] - 0s 81us/step\n",
            "128000/128000 [==============================] - 10s 79us/step\n",
            "Acc Test :  0.8354506957408376\n",
            "Acc Train :  0.5644609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShXqOJL47hCE",
        "colab_type": "text"
      },
      "source": [
        "# Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6AbLWccANuo",
        "colab_type": "code",
        "outputId": "62ca62f1-de76-41fc-867e-1259e9ea2242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "modulation_order = []\n",
        "modulation_order_dict = dict()\n",
        "\n",
        "for key,value in output.items():\n",
        "    modulation_order_dict[np.argmax(value)] = str(key)\n",
        "    \n",
        "for i in range(8):\n",
        "    modulation_order.append(modulation_order_dict[i])\n",
        "    \n",
        "acc_test = classifier.evaluate(X_test18, y_test18)[1]\n",
        "acc_train = classifier.evaluate(X_train, y_train)[1]\n",
        "\n",
        "print(\"Acc Test : \", acc_test)\n",
        "print(\"Acc Train : \", acc_train)    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1653/1653 [==============================] - 0s 172us/step\n",
            "128000/128000 [==============================] - 17s 130us/step\n",
            "Acc Test :  0.8554143981001843\n",
            "Acc Train :  0.579328125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv5RzFyj_Cj1",
        "colab_type": "code",
        "outputId": "fa722349-8bcd-4cb4-f9a0-57cc32cf6c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dic = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "acc = []\n",
        "\n",
        "for snr in snrList:\n",
        "  print('SNR: ', snr)\n",
        "    \n",
        "  y_test_snr = []\n",
        "  X_test_snr = []\n",
        "  for i in range(X_test.shape[0]):\n",
        "    if X_test[i,0,0] == int(snr):\n",
        "      X_test_snr.append(X_test[i])\n",
        "      y_test_snr.append(y_test[i])\n",
        "        \n",
        "  X_test_snr = np.array(X_test_snr)\n",
        "  y_test_snr = np.array(y_test_snr)        \n",
        "  X_test_snr = X_test_snr[:,:,1:]\n",
        "  \n",
        "  X_test_snr = X_test_snr.reshape([X_test_snr.shape[0],256])\n",
        "  X_test_snr = sc.transform(X_test_snr)\n",
        "  X_test_snr = X_test_snr.reshape([X_test_snr.shape[0],2,128])\n",
        "  \n",
        "  X_test_snr = X_test_snr.reshape(-1,2, 128, 1) # For CNN\n",
        "    \n",
        "  acc_test = classifier.evaluate(X_test_snr, y_test_snr)[1]\n",
        "  acc.append(acc_test)\n",
        "  print(acc_test)\n",
        "  '''\n",
        "  y_pred = classifier.predict(X_test_snr)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "  y_real = np.argmax(y_test_snr, axis=1)\n",
        "  # Making the Confusion Matrix\n",
        "  cm = confusion_matrix(y_real, y_pred)\n",
        "  cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    \n",
        "  cmDataFrame = pd.DataFrame(cm_norm, index=modulation_order, columns = modulation_order)\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  ax = sns.heatmap(cmDataFrame, annot=True, annot_kws={\"size\": 8}, fmt='.2f', linewidths=.5, cmap=\"Blues\")\n",
        "\n",
        "  plt.title(f\"CNN0[128(1,7)-64(2,5)] Confusion Matrix (SNR={snr})\")\n",
        "  plt.xlabel(\"Predicted label  \\n\\n TrainAcc={:.2}, TestAcc={:.2}\".format(acc_train,acc_test), fontsize=8)\n",
        "  plt.ylabel(\"True lable\", fontsize=8)\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\", fontsize=8)\n",
        "  plt.setp(ax.get_yticklabels(), fontsize=8)\n",
        "  fig = ax.get_figure()\n",
        "\n",
        "  fig.savefig(dic+f\"/Pic/CNN/CNN0-128(1,7)-64(2,5)-ba{batch}-ep{epoch}-(dout=0.5)-input(IQ)(SNR={snr}).png\", dpi=175, bbox_inches='tight')\n",
        "  fig.savefig(dic+f\"/Pic/CNN/CNN0-128(1,7)-64(2,5)-ba{batch}-ep{epoch}-(dout=0.5)-input(IQ)(SNR={snr}).eps\", bbox_inches='tight')\n",
        "  print(\"Plot Saved!\")\n",
        "'''\n",
        "\n",
        "  del(y_test_snr)\n",
        "  del(X_test_snr)\n",
        "      \n",
        "print(acc)    \n",
        "    "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR:  -20\n",
            "1636/1636 [==============================] - 0s 138us/step\n",
            "0.12163814180929096\n",
            "SNR:  -18\n",
            "1630/1630 [==============================] - 0s 155us/step\n",
            "0.1122699386777322\n",
            "SNR:  -16\n",
            "1606/1606 [==============================] - 0s 139us/step\n",
            "0.1320049813200498\n",
            "SNR:  -14\n",
            "1633/1633 [==============================] - 0s 131us/step\n",
            "0.12614819350887937\n",
            "SNR:  -12\n",
            "1567/1567 [==============================] - 0s 157us/step\n",
            "0.18251435859004023\n",
            "SNR:  -10\n",
            "1561/1561 [==============================] - 0s 146us/step\n",
            "0.2261370915793059\n",
            "SNR:  -8\n",
            "1605/1605 [==============================] - 0s 135us/step\n",
            "0.31588785046728973\n",
            "SNR:  -6\n",
            "1605/1605 [==============================] - 0s 127us/step\n",
            "0.42679127733284067\n",
            "SNR:  -4\n",
            "1600/1600 [==============================] - 0s 130us/step\n",
            "0.528125\n",
            "SNR:  -2\n",
            "1642/1642 [==============================] - 0s 134us/step\n",
            "0.6693057246533439\n",
            "SNR:  0\n",
            "1570/1570 [==============================] - 0s 134us/step\n",
            "0.7821656050955414\n",
            "SNR:  2\n",
            "1577/1577 [==============================] - 0s 138us/step\n",
            "0.830691185833611\n",
            "SNR:  4\n",
            "1555/1555 [==============================] - 0s 149us/step\n",
            "0.8456591637954834\n",
            "SNR:  6\n",
            "1598/1598 [==============================] - 0s 125us/step\n",
            "0.832916145405274\n",
            "SNR:  8\n",
            "1568/1568 [==============================] - 0s 128us/step\n",
            "0.8367346938775511\n",
            "SNR:  10\n",
            "1542/1542 [==============================] - 0s 130us/step\n",
            "0.8527885862516212\n",
            "SNR:  12\n",
            "1598/1598 [==============================] - 0s 130us/step\n",
            "0.8416770966688593\n",
            "SNR:  14\n",
            "1616/1616 [==============================] - 0s 133us/step\n",
            "0.8428217821782178\n",
            "SNR:  16\n",
            "1638/1638 [==============================] - 0s 126us/step\n",
            "0.8565323565323565\n",
            "SNR:  18\n",
            "1653/1653 [==============================] - 0s 129us/step\n",
            "0.8554143981001843\n",
            "[0.12163814180929096, 0.1122699386777322, 0.1320049813200498, 0.12614819350887937, 0.18251435859004023, 0.2261370915793059, 0.31588785046728973, 0.42679127733284067, 0.528125, 0.6693057246533439, 0.7821656050955414, 0.830691185833611, 0.8456591637954834, 0.832916145405274, 0.8367346938775511, 0.8527885862516212, 0.8416770966688593, 0.8428217821782178, 0.8565323565323565, 0.8554143981001843]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_d7UlrRJTdJ",
        "colab_type": "text"
      },
      "source": [
        "# Plot accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofyw4cqWJZQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))  \n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvZtjPOeJadx",
        "colab_type": "code",
        "outputId": "5baecf76-4ccd-4d5f-9c32-c43305ec564b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "plot_history(classifier.history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgElBRERCSgpCAiIgElBRERCfx/o5L7/9CsVU8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxrj6/2gn1p7WyD5+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYsdM-_E0JOk",
        "colab_type": "text"
      },
      "source": [
        "Laod Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YwGx5eVtNHf",
        "colab_type": "code",
        "outputId": "b4b4dfc6-89c8-4fa2-912e-d13005e9258f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "dic = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "model = load_model(dic+f'SaveModel/CNN0-128(1,5)-64(2,5)-ba{batch}-ep{epoch}-(dout=0.5)-input(IQ)(SNR=all).h5')\n",
        "\n",
        "print('Model loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pAIUxXfoxJD",
        "colab_type": "code",
        "outputId": "bb04d472-266b-4c15-fc0e-c17cfeed44dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifier = model\n",
        "model.get_config()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers': [{'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'batch_input_shape': (None, 2, 128, 1),\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 128,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
              "     'config': {'distribution': 'uniform',\n",
              "      'mode': 'fan_avg',\n",
              "      'scale': 1.0,\n",
              "      'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (1, 5),\n",
              "    'name': 'conv2d_9',\n",
              "    'padding': 'same',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_13',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.5,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 64,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
              "     'config': {'distribution': 'uniform',\n",
              "      'mode': 'fan_avg',\n",
              "      'scale': 1.0,\n",
              "      'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (2, 5),\n",
              "    'name': 'conv2d_10',\n",
              "    'padding': 'same',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_14',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.5,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Flatten',\n",
              "   'config': {'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'flatten_5',\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
              "     'config': {'distribution': 'uniform',\n",
              "      'mode': 'fan_avg',\n",
              "      'scale': 1.0,\n",
              "      'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_9',\n",
              "    'trainable': True,\n",
              "    'units': 128,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_15',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.5,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'softmax',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
              "     'config': {'distribution': 'uniform',\n",
              "      'mode': 'fan_avg',\n",
              "      'scale': 1.0,\n",
              "      'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_10',\n",
              "    'trainable': True,\n",
              "    'units': 8,\n",
              "    'use_bias': True}}],\n",
              " 'name': 'sequential_5'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}